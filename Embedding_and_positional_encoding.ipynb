{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a1aa5e-ba70-4fcc-ba8a-15c0ed1f6f82",
   "metadata": {},
   "source": [
    "## At first in the transformer layer we need to create the embedding layer\n",
    "### Embedding Layer which means the converting the input into the presentation of vectors with the expected dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c31e4ac-9bcd-4d4f-8769-f7e4cdd14e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: tensor([[2, 5, 7, 3, 1]])\n",
      "Embedded Sequence: tensor([[[ 1.0937, -0.2105,  0.7825,  ...,  0.6322,  0.9590,  1.1196],\n",
      "         [-0.0424, -1.2583,  1.5339,  ..., -2.4728,  1.6406,  1.2766],\n",
      "         [-0.2880,  0.2182, -2.3618,  ...,  0.2940,  0.3400, -0.9175],\n",
      "         [-0.2165, -0.8007,  2.1178,  ..., -0.0548,  0.8152,  0.2842],\n",
      "         [ 0.2439, -2.5718,  0.7419,  ..., -1.9673,  0.5983, -0.0951]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "vocab_size = 10000  \n",
    "embedding_dim = 512  \n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "input_sequence = torch.tensor([[2, 5, 7, 3, 1]]) \n",
    "embedded_sequence = embedding(input_sequence)\n",
    "print(\"Input Sequence:\", input_sequence)\n",
    "print(\"Embedded Sequence:\", embedded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ab74c-f4b7-4f5b-a5f8-53e8945f7ba3",
   "metadata": {},
   "source": [
    "# Concept of Positional Encoding\n",
    "## In the Transformer architecture, there is no inherent sequential structure in the model, unlike RNNs or LSTMs. To address this, positional encoding is used to inject information about the position of tokens in the sequence into the model. This allows the model to understand the order of tokens.\n",
    "\n",
    "## Formula for Positional Encoding\n",
    "## The positional encoding for each position \n",
    "ð‘ ----> p and each dimension \n",
    "ð‘– ----> i is computed using the following formulas:\n",
    "\n",
    "## Even dimensions (sine function):\n",
    "\n",
    "PE\n",
    "(\n",
    "ð‘\n",
    ",\n",
    "2\n",
    "ð‘–\n",
    ")\n",
    "=\n",
    "sin\n",
    "â¡\n",
    "(\n",
    "ð‘\n",
    "1000\n",
    "0\n",
    "2\n",
    "ð‘–\n",
    "/\n",
    "ð‘‘\n",
    "model\n",
    ")\n",
    "PE(p,2i)=sin( \n",
    "10000 \n",
    "2i/d \n",
    "model\n",
    "â€‹\n",
    " \n",
    " \n",
    "p\n",
    "â€‹\n",
    " )\n",
    "Odd dimensions (cosine function):\n",
    "\n",
    "PE\n",
    "(\n",
    "ð‘\n",
    ",\n",
    "2\n",
    "ð‘–\n",
    "+\n",
    "1\n",
    ")\n",
    "=\n",
    "cos\n",
    "â¡\n",
    "(\n",
    "ð‘\n",
    "1000\n",
    "0\n",
    "2\n",
    "ð‘–\n",
    "/\n",
    "ð‘‘\n",
    "model\n",
    ")\n",
    "PE(p,2i+1)=cos( \n",
    "10000 \n",
    "2i/d \n",
    "model\n",
    "â€‹\n",
    " \n",
    " \n",
    "p\n",
    "â€‹\n",
    " )\n",
    "## Where:\n",
    "\n",
    "ð‘ ----> p is the position of the token in the sequence.\n",
    "ð‘– ----> i is the dimension index.\n",
    "ð‘‘ ----> model\n",
    "d ----> d is the dimensionality of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f6f2fb-b85f-490c-9718-8f8d82b52e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length #Maximum Sequence Length\n",
    "        self.d_model = d_model # Dimensions of the model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float() \n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107724f-d93c-4196-9098-2ae675b0b3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
